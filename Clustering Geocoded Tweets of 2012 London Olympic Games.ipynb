{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Geocoded Tweets of 2012 London Olympic Games\n",
    "by [Talha Oz](http://talhaoz.com) (submitted as GeoSocial Class Assignment #3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "I use DBSCAN clustering, a density based algorithm, to cluster the tweets based on their geographic locations. To do so, I first compute the distances between every unique pair of tweeting points using Vincenty algorithm implemented in geopy, and fit and predict clusters on this distance matrix using sklearn's DBSCAN implementation.\n",
    "\n",
    "Since the locations of the tweeps are scattered around several countries, by ignoring the potential clusters with less than 10 tweeps, we are particularly interested in highly densed clusters of tweeps, where DBSCAN is known to be good in detecting.\n",
    "\n",
    "#### Q2\n",
    "1. Read in the csv file into a dataframe by assigning column names, as there is no header in the provided CSV file.\n",
    "2. Group the tweets by their coordinates [exact (lat,lon) pairs]:\n",
    " 1. Average the sentiment polarities\n",
    " 2. Count number of tweets in each group\n",
    "3. It is interesting that 1778 of 5729 tweets are from same location, i.e. London city center.\n",
    "4. Higher eps, lower min_samples enable us to have low density clusters. Default values were 0.5 and 5, but changed to 15 and 10, respectively.\n",
    "5. Cluster sizes examined and a colormap is selected accordingly\n",
    "6. Folium (Leaflet.js) library is used for interactive mapping where locations are marked in circles whose radii are proportional to tweets originated from the same lat,lon\n",
    "\n",
    "#### Q3\n",
    "* Five clusters are detected.\n",
    "* 286 points (of 1778) could not be detected.\n",
    "* Performed bad in not so densed regions.\n",
    "\n",
    "#### Q4\n",
    "Please see below, an interactive map is provided as the output of the last command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from geopy.distance import vincenty\n",
    "import numpy as np\n",
    "import folium\n",
    "from palettable.colorbrewer.qualitative import Dark2_6\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vincenty_mi(p1,p2):\n",
    "    return vincenty((p1[0],p1[1]),(p2[0],p2[1])).miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 5729\n",
      "Location with the highest tweet count (London city center):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>sp</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>51.506325</td>\n",
       "      <td>-0.127144</td>\n",
       "      <td>0.762092</td>\n",
       "      <td>1778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lat       lon        sp   cnt\n",
       "364  51.506325 -0.127144  0.762092  1778"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Olympic_torch_2012_UK.csv',header=None,names=['twtime','lat','lon','sp'],parse_dates=[0])\n",
    "df['cnt'] = 0\n",
    "# average sp (sentiment polarities) and count tweets from the same lat/lon\n",
    "df = pd.DataFrame(df.groupby(by=['lat','lon'],as_index=False).agg({'cnt':len,'sp':np.mean}))\n",
    "print('Total number of tweets:',df['cnt'].sum())\n",
    "print('Location with the highest tweet count (London city center):')\n",
    "df[df.cnt == df['cnt'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this takes about 1 min 14 secs (measured by %timeit -n1 -r1)...\n",
    "X = pairwise_distances(df[['lat','lon']],metric=vincenty_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>sp</th>\n",
       "      <th>cnt</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.126862</td>\n",
       "      <td>3.429990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.211401</td>\n",
       "      <td>2.209360</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.289863</td>\n",
       "      <td>3.060979</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46.707375</td>\n",
       "      <td>0.874530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.914511</td>\n",
       "      <td>1.160956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lat       lon        sp  cnt  cluster\n",
       "0  46.126862  3.429990  0.000000    1       -1\n",
       "1  46.211401  2.209360  0.636364   11       -1\n",
       "2  46.289863  3.060979  3.000000    1       -1\n",
       "3  46.707375  0.874530  0.000000    1       -1\n",
       "4  46.914511  1.160956  0.000000    1       -1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = DBSCAN(eps=15,min_samples=10,metric='precomputed').fit_predict(X) # eps=0.3, min_samples=10\n",
    "df['cluster'] = db\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of each cluster: [{0: 572}, {1: 277}, {2: 19}, {3: 15}, {4: 16}, {-1: 286}]\n"
     ]
    }
   ],
   "source": [
    "grouped = df.groupby(by='cluster',as_index=False)\n",
    "print('size of each cluster:',[{k:len(v)} for k,v in grouped.groups.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this cell can be removed as the cluster IDs are in the range of [-1,numOfClusters-1]\n",
    "# so, instead of colors[x['cluster']], we could directly use Dark2_6.hex_colors[x['cluster']]\n",
    "colors = {}\n",
    "for i,c in enumerate(set(df['cluster'])):\n",
    "    colors.update({c:Dark2_6.hex_colors[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"uk.html\" style=\"width: 100%; height: 1000px; border: none\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk = folium.Map(location=[53.3, -3.5], zoom_start=7,  width=991, height = 1000)\n",
    "df.apply(lambda x: uk.circle_marker(location=[x['lat'], x['lon']],\n",
    "                 radius=x['cnt']*10,\n",
    "                 popup=str(x['cluster']), line_color=colors[x['cluster']],\n",
    "                 fill_color=colors[x['cluster']], fill_opacity=0.2),\n",
    "         axis=1);\n",
    "uk.create_map(path='uk.html')\n",
    "HTML('<iframe src=\"uk.html\" style=\"width: 100%; height: 1000px; border: none\"></iframe>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
